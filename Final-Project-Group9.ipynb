{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba0cdc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- IMPORTS START --\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "# -- IMPORTS END --\n",
    "\n",
    "# enable zooming into graphs\n",
    "%matplotlib notebook\n",
    "plt.rcParams['figure.figsize'] = [9, 6] # width, height in inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c19f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize model - Do not modify\n",
    "def viz_tree(dt_model,features_frames,cnames):\n",
    "    # Fix feature names as list\n",
    "    feature_names = features_frames.columns.tolist()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9,4))\n",
    "    tree.plot_tree(dt_model,  \n",
    "                   feature_names=feature_names,\n",
    "                   fontsize=7,\n",
    "                   class_names=cnames,\n",
    "                   filled=True,\n",
    "                   ax=ax)\n",
    "\n",
    "    plt.title('Decision Tree')\n",
    "    plt.savefig('dt.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0523e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_magnitude(data):\n",
    "\n",
    "    # Calculate magnitude  \n",
    "    data['accel_mag'] = np.sqrt(data['x']**2 + data['y']**2 + data['z']**2) # absolute accel magnitude\n",
    "    data['accel_mag'] = data['accel_mag'] - data['accel_mag'].mean() # detrend: \"remove gravity\"\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98cdec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(data,sampling_rate):\n",
    "    from scipy.signal import butter, filtfilt, find_peaks\n",
    "\n",
    "    # Low pass filter\n",
    "    cutoff = 5 # Hz\n",
    "    order = 2\n",
    "    b, a = butter(order, cutoff/(sampling_rate/2), btype='lowpass')\n",
    "    data['filtered_accel_mag'] = filtfilt(b, a, data['accel_mag'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d833e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(window):\n",
    "    features = {}\n",
    "    features['avg'] = window['accel_mag'].mean()\n",
    "    features['max'] = window['accel_mag'].quantile(1)\n",
    "    features['med'] = window['accel_mag'].quantile(0.5)\n",
    "    features['min'] = window['accel_mag'].quantile(0)\n",
    "    features['q25'] = window['accel_mag'].quantile(0.25)\n",
    "    features['q75'] = window['accel_mag'].quantile(0.75)\n",
    "    features['std'] = window['accel_mag'].std()\n",
    "    features['var'] = window['accel_mag'].var()\n",
    "    df = pd.DataFrame()\n",
    "    df = df.append(features,ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47a70af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(frames):\n",
    "    # #original code\n",
    "    # # Extract feature columns \n",
    "    # X = frames[['avg', 'max', 'med', 'min', 'q25', 'q75', 'std']]\n",
    "\n",
    "    # # Extract target column\n",
    "    # y = frames['activity']\n",
    "\n",
    "    #split DataFrame into a features matrix X and a labels vector y\n",
    "    # drops the last column in the dataframe\n",
    "    X = frames.iloc[:, :-1]\n",
    "    # X = frames.drop(\"activity\", axis = 'columns')\n",
    "    y = frames['activity']\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) \n",
    "\n",
    "    # Create model\n",
    "    dt_model = DecisionTreeClassifier(criterion='entropy',max_depth=5).fit(X_train, y_train)\n",
    "    dt_pred = dt_model.predict(X_test)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    acc = dt_model.score(X_test, y_test)\n",
    "    dt_cm = confusion_matrix(y_test, dt_pred, labels=dt_model.classes_)\n",
    "    print(classification_report(y_test, dt_pred))\n",
    "    print(\"Accuracy on test set:\", acc)\n",
    "\n",
    "    return dt_model,dt_cm,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2bb66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_live_window(df):\n",
    "    \n",
    "    # Filter accelerometer data \n",
    "    df_accel = df[df['accel_x'].notna() & df['accel_y'].notna() & df['accel_z'].notna()]\n",
    "    df_valid = df_accel[['accel_x', 'accel_y', 'accel_z']].rename(columns={\n",
    "      'accel_x': 'x',\n",
    "      'accel_y': 'y',\n",
    "      'accel_z': 'z'  \n",
    "    })\n",
    "\n",
    "    # Calculate accel_mag\n",
    "    df_valid = calc_magnitude(df_valid) \n",
    "\n",
    "    # Add features\n",
    "    df_valid = add_features(df_valid) \n",
    "    X = df_valid[['avg', 'max', 'med', 'min', 'q25', 'q75',  'std']] \n",
    "\n",
    "    # Load model\n",
    "    with open('dt_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        \n",
    "    # Make prediction\n",
    "    y_pred = model.predict(df_valid)\n",
    "\n",
    "    return(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4142595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_live_classification(): # Testing the live model\n",
    "    # Generate sample DataFrame\n",
    "    data = {'accel_x': [0.011531], \n",
    "            'accel_y': [0.002931],\n",
    "            'accel_z': [0.019604],\n",
    "            'time': ['2023-08-01 18:40:43.344408']}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Repeat rows to get 1000 rows\n",
    "    df = pd.concat([df]*1000, ignore_index=True) \n",
    "\n",
    "    # Call function\n",
    "    y_pred = classify_live_window(df)\n",
    "\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ad9a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract windows and features \n",
    "def extract_features(data, window_sec, sample_rate, activity):\n",
    "\n",
    "    #Aditi's code \n",
    "\n",
    "    STEP_MAX = window_sec\n",
    "    STEP_MIN = 4\n",
    "    VAR_THRESHOLD = 7\n",
    "    \n",
    "    # TODO - see instructions above\n",
    "    frame = pd.DataFrame()\n",
    "    data = calc_magnitude(data)\n",
    "    data = remove_noise(data, sample_rate)\n",
    "    \n",
    "    resampled = data.resample(f'{window_sec}S')\n",
    "    \n",
    "    for window_activity, window_data in resampled:\n",
    "        features = add_features(window_data)\n",
    "        var = features['var']\n",
    "        \n",
    "        peaks, _ = find_peaks(data['filtered_accel_mag'], height=1.5, prominence=0.1, distance=10)\n",
    "        detected_peaks = len(peaks)\n",
    "        if (detected_peaks > STEP_MAX) & (var > VAR_THRESHOLD):\n",
    "            activity = 'falling'\n",
    "        else:\n",
    "            activity = 'not falling'\n",
    "        \n",
    "        features['activity'] = activity\n",
    "        frame = frame.append(features, ignore_index = True)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edd9717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data_to_combined_csv():\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    # TODO - see instructions above\n",
    "    # there is nothing to return from this function. \n",
    "    # The function is writing something to a file instead.\n",
    "    window_sec = 10\n",
    "    sample_rate = 100\n",
    "    \n",
    "    # initializes dataframe\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    # list of activites in the data that we are iterating through\n",
    "    activities = [\"downstairs\", \"jogging\", \"lying\", \"sitting\", \"standing\", \"upstairs\", \"walk_fast\", \"walk_mod\", \"walk_slow\"]\n",
    "\n",
    "    for activity in activities:\n",
    "\n",
    "        # getting the path to the specific activity's folder\n",
    "        path = 'data/Activities'\n",
    "        file_path = os.path.join(path, activity, '*.csv')\n",
    "    \n",
    "        # only conglomerates the files from the specific activity passed in\n",
    "        files = glob.glob(file_path)\n",
    "\n",
    "    \n",
    "        for filename in files:\n",
    "            data = pd.read_csv(filename)\n",
    "    \n",
    "            data = calc_magnitude(data)\n",
    "            data = remove_noise(data, sample_rate)\n",
    "\n",
    "            # same code from last time to convert timestamps\n",
    "            data['time'] = pd.to_datetime(data['time'])\n",
    "            data.set_index('time', inplace = True)\n",
    "    \n",
    "            # extracts activity from data\n",
    "            activity = os.path.basename(os.path.dirname(filename))\n",
    "    \n",
    "            features = extract_features(data, window_sec, sample_rate, activity)\n",
    "    \n",
    "            all_data = all_data.append(features, ignore_index = True)\n",
    "\n",
    "    \n",
    "    # writing dataframe to csv file\n",
    "    all_data.to_csv('all_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c981814",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'RangeIndex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a new dataset that extracts features from all the files and labels them with the corresponding activity\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This function will only create the all_data.csv file once. If you want to overwrite, delete the file first.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#all_data_to_combined_csv()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# make sure path is correct here \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# had to call extract features because we are no longer calling all_data_to_combined_csv()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m feature_frames \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/fall-detection-training-data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m feature_frames \u001b[38;5;241m=\u001b[39m extract_features(feature_frames, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalling\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Activities to drop - pick a subset of the below activities to drop and see how accuracy changes\u001b[39;00m\n\u001b[1;32m     16\u001b[0m all_activities \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalling\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot falling\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(data, window_sec, sample_rate, activity)\u001b[0m\n\u001b[1;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m calc_magnitude(data)\n\u001b[1;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m remove_noise(data, sample_rate)\n\u001b[0;32m---> 15\u001b[0m resampled \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mresample(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_sec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m window_activity, window_data \u001b[38;5;129;01min\u001b[39;00m resampled:\n\u001b[1;32m     18\u001b[0m     features \u001b[38;5;241m=\u001b[39m add_features(window_data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:11392\u001b[0m, in \u001b[0;36mDataFrame.resample\u001b[0;34m(self, rule, axis, closed, label, convention, kind, loffset, base, on, level, origin, offset, group_keys)\u001b[0m\n\u001b[1;32m  11375\u001b[0m \u001b[38;5;129m@doc\u001b[39m(NDFrame\u001b[38;5;241m.\u001b[39mresample, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_shared_doc_kwargs)\n\u001b[1;32m  11376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresample\u001b[39m(\n\u001b[1;32m  11377\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11390\u001b[0m     group_keys: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m lib\u001b[38;5;241m.\u001b[39mNoDefault \u001b[38;5;241m=\u001b[39m no_default,\n\u001b[1;32m  11391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Resampler:\n\u001b[0;32m> 11392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mresample(\n\u001b[1;32m  11393\u001b[0m         rule\u001b[38;5;241m=\u001b[39mrule,\n\u001b[1;32m  11394\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m  11395\u001b[0m         closed\u001b[38;5;241m=\u001b[39mclosed,\n\u001b[1;32m  11396\u001b[0m         label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[1;32m  11397\u001b[0m         convention\u001b[38;5;241m=\u001b[39mconvention,\n\u001b[1;32m  11398\u001b[0m         kind\u001b[38;5;241m=\u001b[39mkind,\n\u001b[1;32m  11399\u001b[0m         loffset\u001b[38;5;241m=\u001b[39mloffset,\n\u001b[1;32m  11400\u001b[0m         base\u001b[38;5;241m=\u001b[39mbase,\n\u001b[1;32m  11401\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[1;32m  11402\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m  11403\u001b[0m         origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m  11404\u001b[0m         offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[1;32m  11405\u001b[0m         group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[1;32m  11406\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:8858\u001b[0m, in \u001b[0;36mNDFrame.resample\u001b[0;34m(self, rule, axis, closed, label, convention, kind, loffset, base, on, level, origin, offset, group_keys)\u001b[0m\n\u001b[1;32m   8855\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_resampler\n\u001b[1;32m   8857\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8858\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_resampler(\n\u001b[1;32m   8859\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8860\u001b[0m     freq\u001b[38;5;241m=\u001b[39mrule,\n\u001b[1;32m   8861\u001b[0m     label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[1;32m   8862\u001b[0m     closed\u001b[38;5;241m=\u001b[39mclosed,\n\u001b[1;32m   8863\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   8864\u001b[0m     kind\u001b[38;5;241m=\u001b[39mkind,\n\u001b[1;32m   8865\u001b[0m     loffset\u001b[38;5;241m=\u001b[39mloffset,\n\u001b[1;32m   8866\u001b[0m     convention\u001b[38;5;241m=\u001b[39mconvention,\n\u001b[1;32m   8867\u001b[0m     base\u001b[38;5;241m=\u001b[39mbase,\n\u001b[1;32m   8868\u001b[0m     key\u001b[38;5;241m=\u001b[39mon,\n\u001b[1;32m   8869\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   8870\u001b[0m     origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m   8871\u001b[0m     offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[1;32m   8872\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[1;32m   8873\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/resample.py:1544\u001b[0m, in \u001b[0;36mget_resampler\u001b[0;34m(obj, kind, **kwds)\u001b[0m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;124;03mCreate a TimeGrouper and return our resampler.\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m tg \u001b[38;5;241m=\u001b[39m TimeGrouper(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m-> 1544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tg\u001b[38;5;241m.\u001b[39m_get_resampler(obj, kind\u001b[38;5;241m=\u001b[39mkind)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/resample.py:1725\u001b[0m, in \u001b[0;36mTimeGrouper._get_resampler\u001b[0;34m(self, obj, kind)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ax, TimedeltaIndex):\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TimedeltaIndexResampler(\n\u001b[1;32m   1722\u001b[0m         obj, groupby\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_keys\n\u001b[1;32m   1723\u001b[0m     )\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1726\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly valid with DatetimeIndex, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimedeltaIndex or PeriodIndex, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1728\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got an instance of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ax)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1729\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'RangeIndex'"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a new dataset that extracts features from all the files and labels them with the corresponding activity\n",
    "# This function will only create the all_data.csv file once. If you want to overwrite, delete the file first.\n",
    "#all_data_to_combined_csv()\n",
    "\n",
    "#feature_frames = pd.read_csv('data/Activities/all_data.csv')\n",
    "\n",
    "# make sure path is correct here \n",
    "# had to call extract features because we are no longer calling all_data_to_combined_csv()\n",
    "feature_frames = pd.read_csv('data/fall-detection-training-data.csv')\n",
    "feature_frames = extract_features(feature_frames, 10, 100, 'falling')\n",
    "\n",
    "# Activities to drop - pick a subset of the below activities to drop and see how accuracy changes\n",
    "all_activities = ['falling', 'not falling']\n",
    "drop_activities = []\n",
    "\n",
    "\n",
    "# TODO: Invert mask to keep only other rows\n",
    "\n",
    "#feature_frames = feature_frames[~feature_frames['activity'].isin(drop_activities)]\n",
    "\n",
    "\n",
    "# TODO: Train the decision tree with the chosen classes\n",
    "# This function will print out precision/recall/accuracy\n",
    "\n",
    "dt_model, dt_cm, acc = train_decision_tree(feature_frames)\n",
    "\n",
    "\n",
    "# TODO: Save the classifier to disk. The name should be exactly dt_model.pkl\n",
    "\n",
    "with open('dt_model.pkl', 'wb') as file:\n",
    "    pickle.dump(dt_model, file)\n",
    "\n",
    "# TODO: Display the confusion matrix\n",
    "\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=dt_cm, display_labels=feature_frames['activity'].unique())\n",
    "display.plot(cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "# TODO: Visualize the tree\n",
    "cnames = list(set(all_activities) - set(drop_activities))\n",
    "\n",
    "viz_tree(dt_model,feature_frames,cnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b41a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_live_classification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
